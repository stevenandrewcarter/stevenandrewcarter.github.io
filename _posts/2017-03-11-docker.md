---
layout: post
title:  "Injest Log Files with Elasticsearch and Docker"
date:   2017-03-11 10:04:01 +0200
categories: Docker Elasticsearch Logstash
---

So you want to quickly mess around with a Elasticsearch cluster and index some data into the cluster. You may want to 
 see if the data is structured correctly, templates are valid or maybe just explore a set of data. Seems easy enough
until you realize that getting data into Elasticsearch can be a little tedious if you try to insert the documents via
the RESTful API. Especially if you want to reindex the data or tweak the data to suit your particular needs. Luckily a 
couple of simple tools exist to import larger collections of data into Elasticsearch and can even be used to do pre 
processing so that the data is how you want / need it.

This article will help you configure a single Elasticsearch node so that you can test injesting data. We will look at 
two approaches of different complexity and will use Docker to containerize the applications. We will also touch briefly
on the advantages and drawbacks to each approach, as well as look at a custom solution.

# Project Layout

This project requires a couple of tools and will end up having multiple moving parts in order to injest data. To start
it is important 

# Getting some Data

So the first step is to get some data to mess around with for an example. Any data could be used and could even be stored
in different formats if required. For the purposes of this exercise we will use a dataset from [Kaggle](www.kaggle.com). 
While the exact set doesn't really matter I would like to use a set that doesn't take a long time to prepare for injestion. 
So, lets use the [Twitter Dataset](https://www.kaggle.com/speckledpingu/RawTwitterFeeds) dataset for fun. The description 
of how the dataset works is provided on the site, and all we care about is providing a process for our aspiring data 
scientist to push the data into a elasticsearch instance so they can test it.

# Setting up the injestion

Now that we have a dataset, we need a way of injesting the data into the elasticsearch cluster. One way of doing this is 
reading each line of the data (It is a csv file), and putting each line into an index. You could write a custom injestor 
to perform this action, but lets use the tools that already exist. Logstash is a awesome tool for injesting data into 
elasticsearch (And other sources) since it allows use to manipulate the data if we need. It also comes with a good 
collection of inputs and outputs out of the box for use. In our situation we want a csv input and a elasticsearch output. 

The csv input will look something like this

```
input {
  file {
    # Source of the data (Check the Docker Compose file for mounting information)
    path => "/tmp/source_data/sheet1.csv"
    # Based on the documentation we know that sheet1 is a set of therapy bot responses
    type => "therapy_bot"
    start_position => "beginning"
  }

  file {
    # Source of the data (Check the Docker Compose file for mounting information)
    path => "/tmp/source_data/sheet2.csv"
    # Based on the documentation we know that sheet2 is a set of resumes
    type => "resumes"
    start_position => "beginning"
  }
}
```

and the elastic output will look something like this.

```
output {
    elasticsearch {
      hosts => ["elasticsearch"]
      index => "%{type}"
      document_type => "%{type}"
      user => "elastic"
      password => "changeme"
      flush_size => 20000
    }
}
```

# 3. Making a cluster

So, we have a injestion strategy for the data files and now we need something that 
